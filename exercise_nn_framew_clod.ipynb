{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1163150 - Neural Network Framework Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Requirements](#Requirements) \n",
    "  * [Modules](#Python-Modules) \n",
    "  * [Data](#Data)\n",
    "* [Neural Network Framework Example](#Neural-Network-Framework-Example)\n",
    "  * [Network](#NerualNetwork-Class)\n",
    "  * [Layer](#Layers)\n",
    "  * [Activation Function](#Activation-Function)   \n",
    "  * [Loss Function](#Loss-Function)\n",
    "  * [Optimizer](#Optimization-with-SGD)\n",
    "  * [Creating a MNIST Neural Network](#Put-it-all-together)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import numpy as np\n",
    "from deep_teaching_commons.data.fundamentals.mnist import Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto download is active, attempting download\n",
      "mnist data directory already exists, download aborted\n",
      "(60000, 28, 28) (60000,)\n",
      "(60000, 1, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "# create mnist loader from deep_teaching_commons\n",
    "mnist_loader = Mnist(data_dir='data')\n",
    "\n",
    "# load all data, labels are one-hot-encoded, images are flatten and pixel squashed between [0,1]\n",
    "train_images, train_labels, test_images, test_labels = mnist_loader.get_all_data(flatten=False, one_hot_enc=False, normalized=True)\n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# reshape to match generel framework architecture \n",
    "train_images, test_images = train_images.reshape(60000, 1, 28, 28), test_images.reshape(10000, 1, 28, 28)            \n",
    "print(train_images.shape, train_labels.shape)\n",
    "\n",
    "# shuffle training data\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "train_images, train_labels = train_images[shuffle_index], train_labels[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_mnist_digit(digit):\n",
    "    image = digit.reshape(28, 28)\n",
    "    plt.imshow(image, cmap='binary', interpolation='bicubic')\n",
    "\n",
    "#choose a random number, plot it and check label \n",
    "random_number = np.random.randint(1,10000)\n",
    "plot_mnist_digit(train_images[random_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Framework Example\n",
    "For a better understanding of neural networks you will start to implement your own framework. The given notebook explaines some core functions and concetps of the framework, so you all have the same starting point. Our previous exercises were self-contained and not very modular. You are going to change that. Let us begin with a fully connected network on the MNIST data set. The  Pipeline will be: \n",
    "\n",
    "**define a model architecture -> construct a neural network from the model -> define a evaluation citeria -> optimize the network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom architecture\n",
    "To create a custom model you have to define layers and activation functions that can be used to do so. Layers and activation functions are modeled as objects. Each object that you want to use has to implement a `forward` and a `backward` method that is used by the `NeuralNetwork` class. Additionally the `self.params` attribute is mandatory to meet the specification of the `NeuralNetwork` class. It is used to store all learnable parameters that you need for the optimization algorithm. Implemented that way you can use the objects as building blocks and stack them up to create a custom model. Be aware of using an activation function after each layer except the last one, cause the softmax function is applied as the default during loss calculation to the network output. \n",
    "\n",
    "### Layers  \n",
    "The file `layer.py` contains implementations of neural network layers and regularization techniques that can be inserted as layers into the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    ''' Flatten layer used to reshape inputs into vector representation\n",
    "    \n",
    "    Layer should be used in the forward pass before a dense layer to \n",
    "    transform a given tensor into a vector. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Reshapes a n-dim representation into a vector \n",
    "            by preserving the number of input rows.\n",
    "        \n",
    "        Args:\n",
    "            X: Images set\n",
    "    \n",
    "        Returns:\n",
    "            X_: Matrix with images in a flatten represenation\n",
    "            \n",
    "        Examples:\n",
    "            [10000,[1,28,28]] -> [10000,784]\n",
    "        '''\n",
    "        self.X_shape = X.shape\n",
    "        self.out_shape = (self.X_shape[0], -1)    \n",
    "        out = X.reshape(-1).reshape(self.out_shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):              \n",
    "        ''' Restore dimensions before flattening operation\n",
    "            \n",
    "        Args:\n",
    "            dout: Matrix from the backprop pass\n",
    "    \n",
    "        Returns:\n",
    "            out: Matrix in the orignal shape\n",
    "            \n",
    "        Examples:\n",
    "            [10000,784] -> [10000,[1,28,28]] \n",
    "        '''\n",
    "        out = dout.reshape(self.X_shape)\n",
    "        return out, []\n",
    "\n",
    "class FullyConnected():\n",
    "    ''' Fully connected layer implemtenting linear function hypothesis \n",
    "        in the forward pass and its derivation in the backward pass.\n",
    "    '''\n",
    "    def __init__(self, in_size, out_size):\n",
    "        ''' Initilize all learning parameters in the layer\n",
    "        \n",
    "        Weights will be initilized with modified Xavier initialization.\n",
    "        Biases will be initilized with zero. \n",
    "        '''\n",
    "        self.W = np.random.randn(in_size, out_size) * np.sqrt(2. / in_size)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):          #IMPLEMENTED 1) \n",
    "        ''' Linear combiationn of images, weights and bias terms\n",
    "            \n",
    "        Args:\n",
    "            X: Matrix of images (flatten represenation)\n",
    "    \n",
    "        Returns:\n",
    "            out: Sum of X*W+b  \n",
    "        '''\n",
    "        self.X = X\n",
    "        #print(\"altezza x \" , len(X))\n",
    "        #print(\"lunghezza x \" , len(X[0]))\n",
    "        #print(\"altezza w \" , len(self.W))\n",
    "        #print(\"lunghezza w \" , len(self.W[0]))\n",
    "        #print(\"bias lunghezza \" ,len(self.b[0]))\n",
    "        #print(\"bias \" , self.b)        \n",
    "        return np.add(np.dot(self.X, self.W), self.b)\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Restore dimensions before flattening operation\n",
    "            \n",
    "        Args:\n",
    "            dout: Derivation of the local out\n",
    "    \n",
    "        Returns:\n",
    "            dX : Derivation with respect to X\n",
    "            dW : Derivation with respect to W\n",
    "            db : Derivation with respect to b\n",
    "        '''\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        dW = np.dot(self.X.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dX, [dW, db]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "The file `activation_func.py` contains implementations of activation functions you can use as a none linearity in your network. The functions work on the basis of matrix operations and not discret values, so that these can also be inserted as a layer. As an example the ReLU function is implemented:\n",
    "\n",
    "$$\n",
    "f ( x ) = \\left\\{ \\begin{array} { l l } { x } & { \\text { if } x > 0 } \\\\ { 0 } & { \\text { otherwise } } \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: DeprecationWarning: invalid escape sequence \\e\n"
     ]
    }
   ],
   "source": [
    "class ReLU():\n",
    "    ''' Implements activation function rectified linear unit (ReLU) \n",
    "    \n",
    "    ReLU activation function is defined as the positive part of \n",
    "    its argument. Todo: insert arxiv paper reference\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, X):           #IMPLEMENTED 2)\n",
    "        ''' In the forward pass return the identity for x < 0\n",
    "        \n",
    "        Safe input for backprop and forward all values that are above 0.\n",
    "        '''\n",
    "        self.X = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ''' Derivative of ReLU\n",
    "        \n",
    "        Retruns:\n",
    "            dX: for all x \\elem X <= 0 in forward pass \n",
    "                return 0 else x\n",
    "            []: no gradients on ReLU operation\n",
    "        '''\n",
    "        dX = dout.copy()\n",
    "        dX[self.X <= 0] = 0\n",
    "        return dX, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    ''' Creates a neural network from a given layer architecture \n",
    "    \n",
    "    This class is suited for fully connected network and\n",
    "    convolutional neural network architectures. It connects \n",
    "    the layers and passes the data from one end to another.\n",
    "    '''\n",
    "    def __init__(self, layers, score_func=None):\n",
    "        ''' Setup a global parameter list and initilize a\n",
    "            score function that is used for predictions.\n",
    "        \n",
    "        Args:\n",
    "            layer: neural network architecture based on layer and activation function objects\n",
    "            score_func: function that is used as classifier on the output\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.score_func = score_func\n",
    "\n",
    "    def forward(self, X):\n",
    "        ''' Pass input X through all layers in the network \n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        ''' Backprop through the network and keep a list of the gradients\n",
    "            from each layer.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Run a forward pass and use the score function to classify \n",
    "            the output.\n",
    "        '''\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(self.score_func(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "Implementations of loss functions can be found in `loss_func.py`. A loss function object defines the criteria your network is evaluated during the optimization process and also contains score functions that can be used as classification criteria for predictions with the final model. Therefore it is necessary to create a loss function object and provide to the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCriteria():\n",
    "    ''' Implements different typs of loss and score functions for neural networks\n",
    "    \n",
    "    Todo:\n",
    "        - Implement init that defines score and loss function \n",
    "    '''\n",
    "    def softmax(X):           #IMPLEMENTED 3)\n",
    "        ''' Numeric stable calculation of softmax\n",
    "        '''    \n",
    "      #  class_scores_max = list(map(lambda x: np.max(x), X))\n",
    "       # e_x2 = []\n",
    "        #for i in range (len(class_scores_max)):\n",
    "         #   temp = np.exp(X[i] - class_scores_max[i])\n",
    "          #  e_x2.append(temp/sum(temp))\n",
    "        exp_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        return  exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_softmax(X, y):\n",
    "        ''' Computes loss and prepares dout for backprop \n",
    "\n",
    "        https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "        '''\n",
    "        #print(\"x shape \", X.shape)\n",
    "        #print(\"y shape \", y.shape)\n",
    "\n",
    "\n",
    "        m = y.shape[0]\n",
    "        p = LossCriteria.softmax(X)\n",
    "        #print(\"m\", m)\n",
    "        #print(\"p \", p)\n",
    "        #print(\"y \", y)\n",
    "        #print(\"p[range(m), y]\", p[range(m), y])\n",
    "        #print(\"p range length \", len(p[range(m), y]))\n",
    "        log_likelihood = -np.log(p[range(m), y])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        dout = p.copy()\n",
    "        dout[range(m), y] -= 1\n",
    "        return loss, dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with SGD\n",
    "The file `optimizer.py` contains implementations of optimization algorithms. Your optimizer needs your custom `network`, `data` and `loss function` and some additional hyperparameter as arguments to optimzie your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():   \n",
    "    def get_minibatches(X, y, batch_size):\n",
    "        ''' Decomposes data set into small subsets (batch)\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        batches = []\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X[i:i + batch_size, :, :, :]\n",
    "            y_batch = y[i:i + batch_size, ]\n",
    "            batches.append((X_batch, y_batch))\n",
    "        return batches    \n",
    "\n",
    "    def sgd(network, X_train, y_train, loss_function, batch_size=32, epoch=100, learning_rate=0.001, X_test=None, y_test=None, verbose=None):\n",
    "        ''' Optimize a given network with stochastic gradient descent \n",
    "        '''\n",
    "        minibatches = Optimizer.get_minibatches(X_train, y_train, batch_size)\n",
    "        for i in range(epoch):\n",
    "            loss = 0\n",
    "            if verbose:\n",
    "                print('\\nEpoch',i + 1)\n",
    "            for X_mini, y_mini in minibatches:\n",
    "                # calculate loss and derivation of the last layer\n",
    "                loss, dout = loss_function(network.forward(X_mini), y_mini)\n",
    "                # calculate gradients via backpropagation\n",
    "                grads = network.backward(dout)\n",
    "                # run vanilla sgd update for all learnable parameters in self.params\n",
    "                for param, grad in zip(network.params, reversed(grads)):\n",
    "                    for i in range(len(grad)):\n",
    "                        param[i] += - learning_rate * grad[i]\n",
    "            if verbose:\n",
    "                train_acc = np.mean(y_train == network.predict(X_train))\n",
    "                test_acc = np.mean(y_test == network.predict(X_test))                                \n",
    "                print(\"Loss = {0} :: Training = {1} :: Test = {2}\".format(loss, train_acc, test_acc))\n",
    "        return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together\n",
    "Now you have parts together to create and train a fully connected neural network. First, you have to define an individual network architecture by flatten the input and stacking fully connected layer with activation functions. Your custom architecture is given to a `NeuralNetwork` object that handles the inter-layer communication during the forward and backward pass. Finally, you optimize the model with a chosen algorithm, here stochastic gradient descent. That kind of pipeline is similar to the one you would create with a framework like Tensorflow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 0.13080349044715595 :: Training = 0.9634166666666667 :: Test = 0.9583\n",
      "Epoch 2\n",
      "Loss = 0.07106590127919123 :: Training = 0.9717833333333333 :: Test = 0.9639\n",
      "Epoch 3\n",
      "Loss = 0.11036261187937392 :: Training = 0.9778 :: Test = 0.9697\n",
      "Epoch 4\n",
      "Loss = 0.12045443999032686 :: Training = 0.9831 :: Test = 0.9701\n",
      "Epoch 5\n",
      "Loss = 0.08572126733702783 :: Training = 0.9860666666666666 :: Test = 0.9737\n",
      "Epoch 6\n",
      "Loss = 0.0014916247266536606 :: Training = 0.99135 :: Test = 0.9761\n",
      "Epoch 7\n",
      "Loss = 0.017862514748481325 :: Training = 0.99175 :: Test = 0.9756\n",
      "Epoch 8\n",
      "Loss = 0.003539296141014636 :: Training = 0.9949 :: Test = 0.9798\n",
      "Epoch 9\n",
      "Loss = 0.0004905129215862725 :: Training = 0.9948166666666667 :: Test = 0.9806\n",
      "Epoch 10\n",
      "Loss = 0.04393780846138685 :: Training = 0.99615 :: Test = 0.9807\n"
     ]
    }
   ],
   "source": [
    "# design a three hidden layer architecture with Dense-Layer\n",
    "# and ReLU as activation function\n",
    "def fcn_mnist():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(784, 500)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(500, 200)\n",
    "    relu_02 = ReLU()\n",
    "    hidden_03 = FullyConnected(200, 100)\n",
    "    relu_03 = ReLU()\n",
    "    ouput = FullyConnected(100, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn = NeuralNetwork(fcn_mnist(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn = Optimizer.sgd(fcn, train_images, train_labels, LossCriteria.cross_entropy_softmax, batch_size=64, \n",
    "                    epoch=10, learning_rate=0.01, X_test=test_images, y_test=test_labels, verbose=True)\n",
    "\n",
    "#TODO cambiare epoch in 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.FullyConnected"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FullyConnected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo: Create your own Architecture on your own data set\n",
    "\n",
    "Your main goal is to explore the given implementation, to perform experiments with different model combinations and to document your observations. So do the following:\n",
    "  * Write a data loader for a different image dataset, e.g., [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) or [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/). Feel free to search a dataset you like to classify.\n",
    "  * Create and train different architectures using thr given implementation.\n",
    "  * Compare the results and document your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 DATASET:\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072) (50000, 10) (10000, 3072) (10000, 10)\n",
      "(50000, 3, 32, 32) (10000, 3, 32, 32)\n",
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from cifar10_web import cifar10\n",
    "\n",
    "train_images2, train_labels2, test_images2, test_labels2 = cifar10(path=None)\n",
    "print(train_images2.shape, train_labels2.shape, test_images2.shape, test_labels2.shape)\n",
    "\n",
    "train_images2, test_images2 = train_images2.reshape(50000, 3, 32, 32), test_images2.reshape(10000, 3, 32, 32)            \n",
    "print(train_images2.shape, test_images2.shape)\n",
    "\n",
    "train_labels2 = np.argmax(train_labels2, axis=1)\n",
    "test_labels2 = np.argmax(test_labels2, axis=1)\n",
    "print(train_labels2.shape)\n",
    "print(test_labels2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 2.345750767463219 :: Training = 0.34848 :: Test = 0.346\n",
      "Epoch 2\n",
      "Loss = 2.293835587536077 :: Training = 0.38404 :: Test = 0.3802\n",
      "Epoch 3\n",
      "Loss = 2.2225507034570113 :: Training = 0.40852 :: Test = 0.4031\n",
      "Epoch 4\n",
      "Loss = 2.1547514765424305 :: Training = 0.41752 :: Test = 0.412\n",
      "Epoch 5\n",
      "Loss = 2.144411988540155 :: Training = 0.42926 :: Test = 0.4253\n",
      "Epoch 6\n",
      "Loss = 2.149600993541519 :: Training = 0.44076 :: Test = 0.4329\n",
      "Epoch 7\n",
      "Loss = 2.1658422083153743 :: Training = 0.4485 :: Test = 0.441\n",
      "Epoch 8\n",
      "Loss = 2.151267864777792 :: Training = 0.45564 :: Test = 0.4487\n",
      "Epoch 9\n",
      "Loss = 2.1535186221072844 :: Training = 0.46452 :: Test = 0.4534\n",
      "Epoch 10\n",
      "Loss = 2.1652569687881766 :: Training = 0.47244 :: Test = 0.4602\n",
      "Epoch 11\n",
      "Loss = 2.1501546001957337 :: Training = 0.47898 :: Test = 0.4639\n",
      "Epoch 12\n",
      "Loss = 2.139880599317618 :: Training = 0.4839 :: Test = 0.4682\n",
      "Epoch 13\n",
      "Loss = 2.1216316137185265 :: Training = 0.489 :: Test = 0.4714\n",
      "Epoch 14\n",
      "Loss = 2.0991703056174815 :: Training = 0.49584 :: Test = 0.476\n",
      "Epoch 15\n",
      "Loss = 2.088321059299114 :: Training = 0.49996 :: Test = 0.479\n",
      "Epoch 16\n",
      "Loss = 2.066098547845189 :: Training = 0.50394 :: Test = 0.4812\n",
      "Epoch 17\n",
      "Loss = 2.059864475467339 :: Training = 0.50714 :: Test = 0.4855\n",
      "Epoch 18\n",
      "Loss = 2.0557675310032777 :: Training = 0.51146 :: Test = 0.4884\n",
      "Epoch 19\n",
      "Loss = 2.035050955810845 :: Training = 0.51454 :: Test = 0.4894\n",
      "Epoch 20\n",
      "Loss = 2.0359113065176673 :: Training = 0.51736 :: Test = 0.4925\n",
      "Epoch 21\n",
      "Loss = 2.015916632406931 :: Training = 0.52004 :: Test = 0.4928\n",
      "Epoch 22\n",
      "Loss = 2.0060042077413294 :: Training = 0.52246 :: Test = 0.4939\n",
      "Epoch 23\n",
      "Loss = 2.0197390428090713 :: Training = 0.52556 :: Test = 0.4954\n",
      "Epoch 24\n",
      "Loss = 2.005247746409971 :: Training = 0.5286 :: Test = 0.496\n",
      "Epoch 25\n",
      "Loss = 2.0259214476023844 :: Training = 0.53022 :: Test = 0.4979\n"
     ]
    }
   ],
   "source": [
    "def fcn_mnist3():\n",
    "    flat = Flatten()\n",
    "    hidden_01 = FullyConnected(3072, 100)\n",
    "    relu_01 = ReLU()\n",
    "    hidden_02 = FullyConnected(100, 50)\n",
    "    relu_02 = ReLU()\n",
    "    #hidden_03 = FullyConnected(50, 25)\n",
    "    #relu_03 = ReLU()\n",
    "    ouput = FullyConnected(50, 10)\n",
    "    return [flat, hidden_01, relu_01, hidden_02, relu_02, ouput]\n",
    "            #hidden_03, relu_03, ouput]\n",
    "\n",
    "# create a neural network on specified architecture with softmax as score function\n",
    "fcn3 = NeuralNetwork(fcn_mnist3(), score_func=LossCriteria.softmax)\n",
    "\n",
    "# optimize the network and a softmax loss\n",
    "fcn3 = Optimizer.sgd(fcn3, train_images2, train_labels2, \n",
    "                     LossCriteria.cross_entropy_softmax, batch_size=64, epoch=10, learning_rate=0.0001, \n",
    "                     X_test=test_images2, y_test=test_labels2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
